# src/llm_lab/config/mac_tinyshakespeare.yaml
data:
  path: "data/raw/shakespeare.txt"

model:
  dim: 256
  n_layers: 4
  n_heads: 8
  vocab_size: 50257 # Default GPT-2 vocab
  block_size: 512
  dropout: 0.1
  rope: 1
  
  # Swappable components
  attn_type: "gqa" # options: mha, gqa, gha, sparse
  n_kv_heads: 2    # for GQA: set to 1 or 2
  mlp_type: "moe" # options: dense, moe
  
  moe:
    num_experts: 4
    top_k: 2
    aux_loss_weight: 0.01

training:
  batch_size: 32
  learning_rate: 6e-4
  max_iters: 5000
  weight_decay: 0.1
  grad_clip: 1.0
  device: "mps" # Change to "cuda" on RunPod
  checkpoint_dir: "checkpoints"
  checkpoint_every: 500

hydra:
  job_logging:
    version: 1
    formatters:
      simple:
        format: "[%(asctime)s][%(name)s][%(levelname)s] - %(message)s"
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.runtime.output_dir}/train.log
    root:
      level: INFO
      handlers: [console, file]
    disable_existing_loggers: false
